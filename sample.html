<!DOCTYPE html> 
<html lang="en">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <title>Qingqing Cao</title>
      <meta name="author" content="Qingqing Cao">
      <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. ">
      <meta name="keywords" content="qingqing cao, research scientist, nlp, ai, efficiency, efficient ai, efficient nlp, llm, efficient llm">
      <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
      <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media id="highlight_theme_light">
      <link rel="shortcut icon" href="/assets/img/prof_pic.jpg">
      <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
      <link rel="canonical" href="https://awk.ai/">
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">
      <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6" type="ed4803c35a774be2cbee485f-text/javascript"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f" type="ed4803c35a774be2cbee485f-text/javascript"></script> 
   </head>
   <body class="fixed-top-nav sticky-bottom-footer">
      <header>
         <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
            <div class="container">
               <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> 
               <div class="collapse navbar-collapse text-right" id="navbarNav">
                  <ul class="navbar-nav ml-auto flex-nowrap">
                     <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li>
                     <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li>
                     <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li>
                     <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li>
                  </ul>
               </div>
            </div>
         </nav>
         <progress id="progress" value="0">
            <div class="progress-container"> <span class="progress-bar"></span> </div>
         </progress>
      </header>
      <div class="container mt-5">
         <div class="post">
            <header class="post-header">
               <h1 class="post-title"> <span class="font-weight-bold">Qingqing</span> Cao </h1>
               <p class="desc">Research Scientist ‚Ä¢ Ô£ø Apple AIML.</p>
            </header>
            <article>
               <div class="profile float-right">
                  <figure>
                     <picture>
                        <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp">
                        </source> 
                        <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp">
                        </source> 
                        <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp">
                        </source> <script src="/cdn-cgi/scripts/7d0fa10a/cloudflare-static/rocket-loader.min.js" data-cf-settings="ed4803c35a774be2cbee485f-|49"></script><img src="/assets/img/prof_pic.jpg?265980fef2fdcbf3c187e7ccc5b5d335" class="img-fluid z-depth-1 rounded-circle" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> 
                     </picture>
                  </figure>
               </div>
               <div class="clearfix">
                  <p>I am a research scientist at Apple AIML. My research interests include efficient NLP, mobile computing, and ML systems. I have focused on building efficient and practical NLP systems for both edge devices and the cloud, such as on-device (visual) question answering and faster Transformer models.</p>
                  <p>Previously, I was a postdoc in the <a href="https://www.cs.washington.edu/research/nlp" rel="external nofollow noopener" target="_blank">UW NLP group</a> at the University of Washington where I won the <a href="https://www.cs.washington.edu/academics/postdoc/research-awards/recipients" rel="external nofollow noopener" target="_blank">postdoc research award</a> twice. I hold a Ph.D. degree in computer science at <a href="https://www.cs.stonybrook.edu/" rel="external nofollow noopener" target="_blank">Stony Brook University</a>. I was a recipient of the <a href="https://www.cs.stonybrook.edu/about-us/News/Funding-Doctoral-Research-Catacosinos-Fellowship-Awardees-2021" rel="external nofollow noopener" target="_blank">Catacosinos Fellowship</a> at Stony Brook University and a <a href="https://datascience.uchicago.edu/research/postdoctoral-programs/rising-stars/2021/" rel="external nofollow noopener" target="_blank">Rising Star in Data Science</a> at the University of Chicago.</p>
               </div>
               <div class="social">
                  <div class="contact-icons"> <a href="/cdn-cgi/l/email-protection#6c495a55495a282c495a5d495b5b495a2e42495a5d495a55" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=vLpPyUUAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/31961604" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/csarron" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/qqcao" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/awk_ai" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fas fa-rss-square"></i></a> </div>
                  <div class="contact-note"> Please use <a href="/cdn-cgi/l/email-protection#73021a10121c331203031f165d101c1e"><span class="__cf_email__" data-cfemail="acddc5cfcdc3eccddcdcc0c982cfc3c1">[email&#160;protected]</span></a> for Apple work, otherwise use <a href="/cdn-cgi/l/email-protection#aac3c7eacbddc184cbc3"><span class="__cf_email__" data-cfemail="a3cacee3c2d4c88dc2ca">[email&#160;protected]</span></a> or <a href="/assets/img/wx.jpg">wechat</a>. </div>
               </div>
               <h2><a href="/news/" style="color: inherit;">News</a></h2>
               <div class="news">
                  <div class="table-responsive" style="max-height: 60vw">
                     <table class="table table-sm table-borderless">
                        <tr>
                           <th scope="row" style="width: 20%;font-family: monospace;">Jun 10, 2024</th>
                           <td> OpenELM is highlighted at <a href="https://developer.apple.com/videos/play/wwdc2024/10223/?time=965" rel="external nofollow noopener" target="_blank">Apple WWDC 2024</a>! Also feel free to check it out on <a href="https://machinelearning.apple.com/research/openelm" rel="external nofollow noopener" target="_blank">Apple Machine Learning Research</a>. </td>
                        </tr>
                        <tr>
                           <th scope="row" style="width: 20%;font-family: monospace;">May 01, 2024</th>
                           <td> <a href="/publications/#zhaoAPTAdaptivePruning2024">APT</a> got accepted to ICML 2024 as an <a href="https://icml.cc/virtual/2024/events/oral#event-35453" style="color: orangered;" rel="external nofollow noopener" target="_blank">oral (1.5%)</a> paper üéâ! Congrats to <a href="https://roim1998.github.io/" rel="external nofollow noopener" target="_blank">Bowen</a> üëè! </td>
                        </tr>
                        <tr>
                           <th scope="row" style="width: 20%;font-family: monospace;">Apr 22, 2024</th>
                           <td> Checkout OpenELM, a new efficient language model family that optimizes parameters for accuracy with fewer tokens using layer-wise scaling! Training code is on <a href="https://github.com/apple/corenet" rel="external nofollow noopener" target="_blank">Github</a>, models are also on <a href="https://huggingface.co/apple/OpenELM" rel="external nofollow noopener" target="_blank">HuggingFace</a>. </td>
                        </tr>
                        <tr>
                           <th scope="row" style="width: 20%;font-family: monospace;">Feb 15, 2024</th>
                           <td> Glad to be invited to serve as Action Editor / Area Chair for ACL 2024 ! </td>
                        </tr>
                        <tr>
                           <th scope="row" style="width: 20%;font-family: monospace;">Jan 16, 2024</th>
                           <td> <a href="/publications/#caoBTRBinaryToken2024">BTR</a> was accepted to ICLR as a <a href="https://iclr.cc/virtual/2024/poster/19511" style="color: orangered;" rel="external nofollow noopener" target="_blank">spotlight (5%)</a> paper! üéä </td>
                        </tr>
                     </table>
                  </div>
               </div>
               <h2><a href="/publications/" style="color: inherit;">Recent publications</a></h2>
               <div class="publications">
                  <h2 class="bibliography">2024</h2>
                  <ol class="bibliography">
                     <li>
                        <div class="row">
                           <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a></abbr></div>
                           <div id="mehtaOpenELMEfficientLanguage2024" class="col-sm-8">
                              <div class="title"> <a href="#mehtaOpenELMEfficientLanguage2024">#</a> OpenELM: An Efficient Language Model Family with Open Training and Inference Framework</div>
                              <div class="author"> <a href="https://sacmehta.github.io/" rel="external nofollow noopener" target="_blank">Sachin Mehta</a>, <a href="https://ca.linkedin.com/in/mhsekhavat" rel="external nofollow noopener" target="_blank">Mohammad Hossein Sekhavat</a>, <em><strong>Qingqing Cao</strong></em>, <a href="https://mchorton.com/" rel="external nofollow noopener" target="_blank">Maxwell Horton</a>, <a href="https://www.linkedin.com/in/yanzi-jin-112a3137" rel="external nofollow noopener" target="_blank">Yanzi Jin</a>, <a href="https://www.linkedin.com/in/chenfansun" rel="external nofollow noopener" target="_blank">Chenfan Sun</a>, <a href="https://imirzadeh.me/" rel="external nofollow noopener" target="_blank">Iman Mirzadeh</a>, <a href="https://www.mahyarnajibi.com/" rel="external nofollow noopener" target="_blank">Mahyar Najibi</a>, <a href="https://www.linkedin.com/in/dmitrybelenko" rel="external nofollow noopener" target="_blank">Dmitry Belenko</a>, <a href="https://www.linkedin.com/in/peterzat" rel="external nofollow noopener" target="_blank">Peter Zatloukal</a>, and <a href="https://mrastegari.github.io/" rel="external nofollow noopener" target="_blank">Mohammad Rastegari</a> </div>
                              <div class="periodical"> Apr 2024 </div>
                              <div class="periodical"> </div>
                              <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2404.14619" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> </div>
                              <div class="badges"> </div>
                              <div class="abstract hidden">
                                 <p>The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, we release OpenELM, a state-of-the-art open language model. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. For example, with a parameter budget of approximately one billion parameters, OpenELM exhibits a 2.36% improvement in accuracy compared to OLMo while requiring 2x fewer pre-training tokens. Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, our release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations. We also release code to convert models to MLX library for inference and fine-tuning on Apple devices. This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors. Our source code along with pre-trained model weights and training recipes is available at https://github.com/apple/corenet. Additionally, OpenELM models can be found on HuggingFace at: https://huggingface.co/apple/OpenELM.</p>
                              </div>
                              <div class="bibtex hidden">
                                 <figure class="highlight">
                                    <pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">mehtaOpenELMEfficientLanguage2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{OpenELM}: {An} {Efficient} {Language} {Model} {Family} with {Open} {Training} and {Inference} {Framework}}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{{OpenELM}}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2024-04-29}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mehta, Sachin and Sekhavat, Mohammad Hossein and Cao, Qingqing and Horton, Maxwell and Jin, Yanzi and Sun, Chenfan and Mirzadeh, Iman and Najibi, Mahyar and Belenko, Dmitry and Zatloukal, Peter and Rastegari, Mohammad}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre>
                                 </figure>
                              </div>
                           </div>
                        </div>
                     </li>
                     <li>
                        <div class="row">
                           <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://icml.cc/Conferences/2024" rel="external nofollow noopener" target="_blank">ICML 2024</a></abbr></div>
                           <div id="zhaoAPTAdaptivePruning2024" class="col-sm-8">
                              <div class="title"> <a href="#zhaoAPTAdaptivePruning2024">#</a> APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference</div>
                              <div class="author"> <a href="https://roim1998.github.io/" rel="external nofollow noopener" target="_blank">Bowen Zhao</a>, <a href="https://homes.cs.washington.edu/~hannaneh/index.html" rel="external nofollow noopener" target="_blank">Hannaneh Hajishirzi</a>, and <em><strong>Qingqing Cao</strong></em> </div>
                              <div class="periodical"> Jan 2024 </div>
                              <div class="periodical"> </div>
                              <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2401.12200" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="https://openreview.net/forum?id=sb81Xl50JG" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://icml.cc/virtual/2024/events/oral#event-35453" class="btn btn-sm z-depth-0" style="font-weight: bold; color: orangered;" rel="external nofollow noopener" target="_blank">Oral (1.5%)</a> </div>
                              <div class="badges"> </div>
                              <div class="abstract hidden">
                                 <p>Fine-tuning and inference with large Language Models (LM) are generally known to be expensive. Parameter-efficient fine-tuning over pretrained LMs reduces training memory by updating a small number of LM parameters but does not improve inference efficiency. Structured pruning improves LM inference efficiency by removing consistent parameter blocks, yet often increases training memory and time. To improve both training and inference efficiency, we introduce APT that adaptively prunes and tunes parameters for the LMs. At the early stage of fine-tuning, APT dynamically adds salient tuning parameters for fast and accurate convergence while discarding unimportant parameters for efficiency. Compared to baselines, our experiments show that APT maintains up to 98% task performance when pruning RoBERTa and T5 models with 40% parameters left while keeping 86.4% LLaMA models‚Äô performance with 70% parameters remained. Furthermore, APT speeds up LMs fine-tuning by up to 8x and reduces large LMs memory training footprint by up to 70%.</p>
                              </div>
                              <div class="bibtex hidden">
                                 <figure class="highlight">
                                    <pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhaoAPTAdaptivePruning2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{{APT}}: {{Adaptive Pruning}} and {{Tuning Pretrained Language Models}} for {{Efficient Training}} and {{Inference}}}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{{{APT}}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhao, Bowen and Hajishirzi, Hannaneh and Cao, Qingqing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=sb81Xl50JG}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{arXiv:2401.12200}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2401.12200}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{arXiv}}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2024-01-23}</span><span class="p">,</span>
  <span class="na">highlight</span> <span class="p">=</span> <span class="s">{Oral (1.5%)}</span><span class="p">,</span>
  <span class="na">highlight_url</span> <span class="p">=</span> <span class="s">{https://icml.cc/virtual/2024/events/oral#event-35453}</span>
<span class="p">}</span></code></pre>
                                 </figure>
                              </div>
                           </div>
                        </div>
                     </li>
                  </ol>
                  <h2 class="bibliography">2023</h2>
                  <ol class="bibliography">
                     <li>
                        <div class="row">
                           <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://iclr.cc/Conferences/2024" rel="external nofollow noopener" target="_blank">ICLR 2024</a></abbr></div>
                           <div id="caoBTRBinaryToken2024" class="col-sm-8">
                              <div class="title"> <a href="#caoBTRBinaryToken2024">#</a> BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models</div>
                              <div class="author"> <em><strong>Qingqing Cao</strong></em>, <a href="https://shmsw25.github.io/" rel="external nofollow noopener" target="_blank">Sewon Min</a>, <a href="https://homes.cs.washington.edu/~yizhongw/" rel="external nofollow noopener" target="_blank">Yizhong Wang</a>, and <a href="https://homes.cs.washington.edu/~hannaneh/index.html" rel="external nofollow noopener" target="_blank">Hannaneh Hajishirzi</a> </div>
                              <div class="periodical"> <em>In </em>, Oct 2023 </div>
                              <div class="periodical"> </div>
                              <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2310.01329" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="https://openreview.net/forum?id=3TO3TtnOFl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="/assets/pdf/BTR-poster-ICLR2024.pptx" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/btr-slides.pptx" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://iclr.cc/virtual/2024/poster/19511" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://iclr.cc/virtual/2024/poster/19511" class="btn btn-sm z-depth-0" style="font-weight: bold; color: orangered;" rel="external nofollow noopener" target="_blank">Spotlight (5%)</a> </div>
                              <div class="badges"> </div>
                              <div class="abstract hidden">
                                 <p>Retrieval augmentation addresses many critical problems in large language models such as hallucination, staleness, and privacy leaks. However, running retrieval-augmented language models (LMs) is slow and difficult to scale due to processing large amounts of retrieved text. We introduce binary token representations (BTR), which use 1-bit vectors to precompute every token in passages, significantly reducing computation during inference. Despite the potential loss of accuracy, our new calibration techniques and training objectives restore performance. Combined with offline and runtime compression, this only requires 127GB of disk space for encoding 3 billion tokens in Wikipedia. Our experiments show that on five knowledge-intensive NLP tasks, BTR accelerates state-of-the-art inference by up to 4x and reduces storage by over 100x while maintaining over 95% task performance. Our code is publicly available at https://github.com/csarron/BTR.</p>
                              </div>
                              <div class="bibtex hidden">
                                 <figure class="highlight">
                                    <pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">caoBTRBinaryToken2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{BTR}: {Binary} {Token} {Representations} for {Efficient} {Retrieval} {Augmented} {Language} {Models}}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{{BTR}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=3TO3TtnOFl}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2024-04-29}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cao, Qingqing and Min, Sewon and Wang, Yizhong and Hajishirzi, Hannaneh}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">highlight</span> <span class="p">=</span> <span class="s">{Spotlight (5%)}</span><span class="p">,</span>
<span class="p">}</span></code></pre>
                                 </figure>
                              </div>
                           </div>
                        </div>
                     </li>
                     <li>
                        <div class="row">
                           <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://2023.aclweb.org/" rel="external nofollow noopener" target="_blank">ACL 2023</a></abbr></div>
                           <div id="cao-etal-2023-pumer" class="col-sm-8">
                              <div class="title"> <a href="#cao-etal-2023-pumer">#</a> PuMer: Pruning and Merging Tokens for Efficient Vision Language Models</div>
                              <div class="author"> <em><strong>Qingqing Cao</strong></em>, <a href="https://bhargaviparanjape.github.io/" rel="external nofollow noopener" target="_blank">Bhargavi Paranjape</a>, and <a href="https://homes.cs.washington.edu/~hannaneh/index.html" rel="external nofollow noopener" target="_blank">Hannaneh Hajishirzi</a> </div>
                              <div class="periodical"> <em>In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, Jul 2023 </div>
                              <div class="periodical"> </div>
                              <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2305.17530" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">BibTeX</a> <a href="https://aclanthology.org/2023.acl-long.721" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://github.com/csarron/PuMer" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/pumer-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/pumer-slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div>
                              <div class="badges"> </div>
                              <div class="abstract hidden">
                                 <p>Large-scale vision language (VL) models use Transformers to perform cross-modal interactions between the input text and image. These cross-modal interactions are computationally expensive and memory-intensive due to the quadratic complexity of processing the input image and text. We present PuMer: a token reduction framework that uses text-informed Pruning and modality-aware Merging strategies to progressively reduce the tokens of input image and text, improving model inference speed and reducing memory footprint. PuMer learns to keep salient image tokens related to the input text and merges similar textual and visual tokens by adding lightweight token reducer modules at several cross-modal layers in the VL model. Training PuMer is mostly the same as finetuning the original VL model but faster. Our evaluation for two vision language models on four downstream VL tasks shows PuMer increases inference throughput by up to 2x and reduces memory footprint by over 50% while incurring less than a 1% accuracy drop.</p>
                              </div>
                              <div class="bibtex hidden">
                                 <figure class="highlight">
                                    <pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">cao-etal-2023-pumer</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{P}u{M}er: Pruning and Merging Tokens for Efficient Vision Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cao, Qingqing and Paranjape, Bhargavi and Hajishirzi, Hannaneh}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Toronto, Canada}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.acl-long.721}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2023.acl-long.721}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{12890--12903}</span><span class="p">,</span>
<span class="p">}</span></code></pre>
                                 </figure>
                              </div>
                           </div>
                        </div>
                     </li>
                  </ol>
               </div>
            </article>
         </div>
      </div>
      <footer class="sticky-bottom mt-5">
         <div class="container" align="center"> ¬© Copyright 2024 Qingqing Cao. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="external nofollow noopener">al-folio</a> theme. Last updated: July, 2024. </div>
      </footer>
      <script data-cfasync="false" src="/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous" type="ed4803c35a774be2cbee485f-text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous" type="ed4803c35a774be2cbee485f-text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous" type="ed4803c35a774be2cbee485f-text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous" type="ed4803c35a774be2cbee485f-text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js" type="ed4803c35a774be2cbee485f-text/javascript"></script> <script defer src="/assets/js/masonry.js" type="ed4803c35a774be2cbee485f-text/javascript"></script> <script type="ed4803c35a774be2cbee485f-text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous" type="ed4803c35a774be2cbee485f-text/javascript"></script> <script defer src="/assets/js/zoom.js" type="ed4803c35a774be2cbee485f-text/javascript"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js" type="ed4803c35a774be2cbee485f-text/javascript"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c" type="ed4803c35a774be2cbee485f-text/javascript"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01" type="ed4803c35a774be2cbee485f-text/javascript"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="ed4803c35a774be2cbee485f-text/javascript"></script> <script type="ed4803c35a774be2cbee485f-text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="ed4803c35a774be2cbee485f-text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" type="ed4803c35a774be2cbee485f-text/javascript"></script> <script type="ed4803c35a774be2cbee485f-text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/cdn-cgi/scripts/7d0fa10a/cloudflare-static/rocket-loader.min.js" data-cf-settings="ed4803c35a774be2cbee485f-|49" defer></script><script>(function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML="window.__CF$cv$params={r:'8c973cba793ea3b6',t:'MTcyNzM5NTQ3NS4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();</script><script defer src="https://static.cloudflareinsights.com/beacon.min.js/vcd15cbe7772f49c399c6a5babf22c1241717689176015" integrity="sha512-ZpsOmlRQV6y907TI0dKBHq9Md29nnaEIPlkf84rnaERnq6zvWvPUqr2ft8M1aS28oN72PdrCzSjY4U6VaAw1EQ==" data-cf-beacon='{"rayId":"8c973cba793ea3b6","version":"2024.8.0","r":1,"token":"3883a19d975c447795814b16b985b97f","serverTiming":{"name":{"cfExtPri":true,"cfL4":true}}}' crossorigin="anonymous"></script>
   </body>
</html>
